{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basepath = './aclImdb'\n",
    "def read_corpus():\n",
    "    for s in ('test', 'train'):\n",
    "        for l in ('pos', 'neg'):\n",
    "            path = os.path.join(basepath, s, l)\n",
    "            for file in os.listdir(path):\n",
    "                with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                    yield preprocessor(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2w_vec = dict()\n",
    "w_vec = dict()\n",
    "nneighbors = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for text in read_corpus():\n",
    "    words = text.split()\n",
    "    for wi in range(len(words)):\n",
    "        w_vec[words[wi]] = w_vec.get(words[wi], 0) + 1\n",
    "        for i in range(wi + 1, wi + 1 + nneighbors):\n",
    "            if len(words) > i:\n",
    "                w_pair = (words[wi], words[i])\n",
    "                w2w_vec[w_pair] = w2w_vec.get(w_pair, 0) + 1 \n",
    "        for i in range(wi - nneighbors, wi):\n",
    "            if i > 0:\n",
    "                w2w_vec[w_pair] = w2w_vec.get(w_pair, 0) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pmi_scoring(w1, w2):\n",
    "    score = w2w_vec[(w1,w2)] / (w_vec[w1] * w_vec[w2])\n",
    "    return math.log2(score) if score else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ppmi_scoring(w1,w2):\n",
    "    return max(pmi_scoring(w1,w2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppmi_vec = dict()\n",
    "for w1, w2 in w2w_vec:\n",
    "    if w1 not in ppmi_vec:\n",
    "        ppmi_vec[w1] = dict()\n",
    "    score = ppmi_scoring(w1,w2)\n",
    "    if score:\n",
    "        ppmi_vec[w1][w2] = score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_similarity(w1,w2):\n",
    "    dot_w1_w2 = 0\n",
    "    for w_other in ppmi_vec[w1]:\n",
    "        if w_other in ppmi_vec[w2]:\n",
    "            dot_w1_w2 += ppmi_vec[w1][w_other] * ppmi_vec[w2][w_other]\n",
    "    vec_len_w1 = math.sqrt(sum([v**2 for v in ppmi_vec[w1].values()]))\n",
    "    vec_len_w2 = math.sqrt(sum([v**2 for v in ppmi_vec[w2].values()]))\n",
    "    return dot_w1_w2 / (vec_len_w1 * vec_len_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppmi_vec_filtered = list(filter(lambda x: ppmi_vec[x], ppmi_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ppmi_vec_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_w = list()\n",
    "for w1 in ppmi_vec_filtered:\n",
    "    for w2 in ppmi_vec_filtered:\n",
    "        if w1 == w2:\n",
    "            continue\n",
    "        sim = cos_similarity(w1, w2)\n",
    "        if sim: \n",
    "             sim_w.append((w1,w2,sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_spiritited', '_toy', 1.0),\n",
       " ('_spiritited', 'away_', 1.0),\n",
       " ('omero', 'antonutti', 1.0),\n",
       " ('friedo', 'almghandi', 1.0),\n",
       " ('jhj', 'nashdnfhcka', 1.0),\n",
       " ('jhj', 'sakasdadj', 1.0),\n",
       " ('jhj', 'fhnkhad', 1.0),\n",
       " ('nargis', 'bagheri', 1.0),\n",
       " ('v1', 'v2', 1.0),\n",
       " ('v1', '20061114', 1.0),\n",
       " ('jpieczanski', 'sidwell', 1.0),\n",
       " ('jpieczanski', 'pieczanski', 1.0),\n",
       " ('kaempfen', 'selbst', 1.0),\n",
       " ('kaempfen', 'goetter', 1.0),\n",
       " ('ld_____________________________my',\n",
       "  'ld______________________________________________my',\n",
       "  1.0),\n",
       " ('ld_____________________________my', 'ld_________my', 1.0),\n",
       " ('924', '310', 1.0),\n",
       " ('dans', 'nos', 1.0),\n",
       " ('dans', 'toujours', 1.0),\n",
       " ('naaaaaaaaaaaaaaah', '555', 1.0),\n",
       " ('wollter', 'ahmed', 1.0),\n",
       " ('wollter', 'selldal', 1.0),\n",
       " ('selbst', 'kaempfen', 1.0),\n",
       " ('selbst', 'goetter', 1.0),\n",
       " ('_toy', '_spiritited', 1.0),\n",
       " ('_toy', 'away_', 1.0),\n",
       " ('nos', 'dans', 1.0),\n",
       " ('nos', 'toujours', 1.0),\n",
       " ('gli', 'occhi', 1.0),\n",
       " ('khandar', 'daar', 1.0),\n",
       " ('oonga', 'dhanno', 1.0),\n",
       " ('knifee', 'likee', 1.0),\n",
       " ('gatesville', 'tokar', 1.0),\n",
       " ('555', 'naaaaaaaaaaaaaaah', 1.0),\n",
       " ('nashdnfhcka', 'jhj', 1.0),\n",
       " ('nashdnfhcka', 'sakasdadj', 1.0),\n",
       " ('nashdnfhcka', 'fhnkhad', 1.0),\n",
       " ('kleber', 'mendon√ßa', 1.0),\n",
       " ('kaiso', 'heidecke', 1.0),\n",
       " ('hasta', 'revolucion', 1.0),\n",
       " ('kno', 'wahm', 1.0),\n",
       " ('kno', 'sayin', 1.0),\n",
       " ('valleyspeak', 'airheadedness', 1.0),\n",
       " ('vo12no18_mccarthy', 'madtrapper', 1.0),\n",
       " ('vo12no18_mccarthy', 'joking_apart_s1', 1.0),\n",
       " ('ruuun', 'saaaaaave', 1.0),\n",
       " ('ruuun', 'awaaaaay', 1.0),\n",
       " ('310', '924', 1.0),\n",
       " ('ld______________________________________________my',\n",
       "  'ld_____________________________my',\n",
       "  1.0),\n",
       " ('ld______________________________________________my', 'ld_________my', 1.0),\n",
       " ('xoxo', 'wolly', 1.0),\n",
       " ('madtrapper', 'vo12no18_mccarthy', 1.0),\n",
       " ('madtrapper', 'joking_apart_s1', 1.0),\n",
       " ('occhi', 'gli', 1.0),\n",
       " ('scheduleservlet', 'focus_id', 1.0),\n",
       " ('scheduleservlet', 'action_detail', 1.0),\n",
       " ('angelfire', 'ny5', 1.0),\n",
       " ('wolly', 'xoxo', 1.0),\n",
       " ('monkschild', 'templechild', 1.0),\n",
       " ('seppuka', 'betterchoice', 1.0),\n",
       " ('v2', 'v1', 1.0),\n",
       " ('v2', '20061114', 1.0),\n",
       " ('sidwell', 'jpieczanski', 1.0),\n",
       " ('sidwell', 'pieczanski', 1.0),\n",
       " ('pieczanski', 'jpieczanski', 1.0),\n",
       " ('pieczanski', 'sidwell', 1.0),\n",
       " ('focus_id', 'scheduleservlet', 1.0),\n",
       " ('focus_id', 'action_detail', 1.0),\n",
       " ('20061114', 'v1', 1.0),\n",
       " ('20061114', 'v2', 1.0),\n",
       " ('dst', 'grandmixer', 1.0),\n",
       " ('gutenberg', 'ebooks', 1.0),\n",
       " ('dhanno', 'oonga', 1.0),\n",
       " ('betterchoice', 'seppuka', 1.0),\n",
       " ('templechild', 'monkschild', 1.0),\n",
       " ('revolucion', 'hasta', 1.0),\n",
       " ('ahmed', 'wollter', 1.0),\n",
       " ('ahmed', 'selldal', 1.0),\n",
       " ('wahm', 'kno', 1.0),\n",
       " ('wahm', 'sayin', 1.0),\n",
       " ('protiv', 'davitelj', 1.0),\n",
       " ('muy', 'bien', 1.0),\n",
       " ('sakasdadj', 'jhj', 1.0),\n",
       " ('sakasdadj', 'nashdnfhcka', 1.0),\n",
       " ('sakasdadj', 'fhnkhad', 1.0),\n",
       " ('journeyed', 'kampung', 1.0),\n",
       " ('gliss', 'annuder', 1.0),\n",
       " ('toujours', 'dans', 1.0),\n",
       " ('toujours', 'nos', 1.0),\n",
       " ('almghandi', 'friedo', 1.0),\n",
       " ('saaaaaave', 'ruuun', 1.0),\n",
       " ('saaaaaave', 'awaaaaay', 1.0),\n",
       " ('action_detail', 'scheduleservlet', 1.0),\n",
       " ('action_detail', 'focus_id', 1.0),\n",
       " ('punted', 'crippler', 1.0),\n",
       " ('fedor', 'miklowitz', 1.0),\n",
       " ('heidecke', 'kaiso', 1.0),\n",
       " ('miklowitz', 'fedor', 1.0),\n",
       " ('joking_apart_s1', 'vo12no18_mccarthy', 1.0),\n",
       " ('joking_apart_s1', 'madtrapper', 1.0),\n",
       " ('likee', 'knifee', 1.0),\n",
       " ('bien', 'muy', 1.0),\n",
       " ('fhnkhad', 'jhj', 1.0),\n",
       " ('fhnkhad', 'nashdnfhcka', 1.0),\n",
       " ('fhnkhad', 'sakasdadj', 1.0),\n",
       " ('annuder', 'gliss', 1.0),\n",
       " ('bagheri', 'nargis', 1.0),\n",
       " ('antonutti', 'omero', 1.0),\n",
       " ('daar', 'khandar', 1.0),\n",
       " ('awaaaaay', 'ruuun', 1.0),\n",
       " ('awaaaaay', 'saaaaaave', 1.0),\n",
       " ('sayin', 'kno', 1.0),\n",
       " ('sayin', 'wahm', 1.0),\n",
       " ('ebooks', 'gutenberg', 1.0),\n",
       " ('goetter', 'kaempfen', 1.0),\n",
       " ('goetter', 'selbst', 1.0),\n",
       " ('ld_________my', 'ld_____________________________my', 1.0),\n",
       " ('ld_________my', 'ld______________________________________________my', 1.0),\n",
       " ('tokar', 'gatesville', 1.0),\n",
       " ('airheadedness', 'valleyspeak', 1.0),\n",
       " ('kampung', 'journeyed', 1.0),\n",
       " ('mendon√ßa', 'kleber', 1.0),\n",
       " ('crippler', 'punted', 1.0),\n",
       " ('davitelj', 'protiv', 1.0),\n",
       " ('ny5', 'angelfire', 1.0),\n",
       " ('selldal', 'wollter', 1.0),\n",
       " ('selldal', 'ahmed', 1.0),\n",
       " ('grandmixer', 'dst', 1.0),\n",
       " ('away_', '_spiritited', 1.0),\n",
       " ('away_', '_toy', 1.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
